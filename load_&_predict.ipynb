{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lazyYC/EasyReadNews/blob/main/load_%26_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 工具準備\n",
        "* clone repository\n",
        "* install & import 需要用到套件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh0P-FIlPJR8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "!git clone 'https://github.com/lazyYC/EasyReadNews.git'\n",
        "os.chdir('./EasyReadNews')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLtK0Gzk3ZOa"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers  rouge-score\n",
        "!pip3 install newspaper3k\n",
        "!pip install 'torch>=1.6.0' editdistance matplotlib sacrebleu sacremoses sentencepiece tqdm wandb\n",
        "!pip install --upgrade jupyter ipywidgets\n",
        "!git clone https://github.com/pytorch/fairseq.git\n",
        "!cd fairseq && git checkout 9a1c497\n",
        "!pip install --upgrade ./fairseq/\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from newspaper import fulltext\n",
        "import requests, sys, random, re, logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import tqdm.auto as tqdm\n",
        "from pathlib import Path\n",
        "from argparse import Namespace\n",
        "from fairseq import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import sentencepiece as spm\n",
        "from config import *\n",
        "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oQSBcGV3mvl"
      },
      "source": [
        "### 載入transformers, newspaper套件\n",
        "* 輸入網址透過 newspaper 抓取新聞內容\n",
        "* 交由 pre-trained model產生summerization\n",
        "* 產出 txt 檔交由後續 translation model 使用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwnLKA8PA8Kq"
      },
      "outputs": [],
      "source": [
        "url = input(\"Please paste the url of the news to be summarized.\")\n",
        "text = fulltext(requests.get(url).text)\n",
        "ARTICLE_TO_SUMMARIZE = text\n",
        "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n",
        "\n",
        "# Generate Summary\n",
        "summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=100, early_stopping=True)\n",
        "output = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids][0]\n",
        "str2 = output.split('.')\n",
        "\n",
        "with open(\"summary_out_put.txt\",\"w+\") as f:\n",
        "  for i in range(len(str2)):\n",
        "    f.write(str2[i])\n",
        "    if i != len(str2) - 1:\n",
        "      f.write('\\n')\n",
        "with open(\"summary_out_put.txt\", \"r\") as f:\n",
        "  data = f.readlines()\n",
        "  for i in data:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 翻譯模型\n",
        "* 引入所需要的tasks, models, config等等\n",
        "* 引入預測的函式"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jqb1YkPZKUNK"
      },
      "outputs": [],
      "source": [
        "## set device\n",
        "## 如果GPU不能用，就把前兩行markdown\n",
        "cuda_env = utils.CudaEnvironment()\n",
        "utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "## setup task\n",
        "task_cfg = TranslationConfig(\n",
        "    data=config.datadir,\n",
        "    source_lang=config.source_lang,\n",
        "    target_lang=config.target_lang,\n",
        "    train_subset=\"train\",\n",
        "    required_seq_len_multiple=8,\n",
        "    dataset_impl=\"mmap\",\n",
        "    upsample_primary=1,\n",
        ")\n",
        "task = TranslationTask.setup_task(task_cfg)\n",
        "\n",
        "##logging\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=\"INFO\", # \"DEBUG\" \"WARNING\" \"ERROR\"\n",
        "    stream=sys.stdout,\n",
        ")\n",
        "proj = \"hw5.seq2seq\"\n",
        "logger = logging.getLogger(proj)\n",
        "if config.use_wandb:\n",
        "    import wandb\n",
        "    wandb.init(project=proj, name=Path(config.savedir).stem, config=config)\n",
        "\n",
        "## seed\n",
        "seed = 73\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  \n",
        "np.random.seed(seed)  \n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot7Pai5qWwGg"
      },
      "outputs": [],
      "source": [
        "def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=1, cached=True):\n",
        "    batch_iterator = task.get_batch_iterator(\n",
        "        dataset=task.dataset(split),\n",
        "        max_tokens=max_tokens,\n",
        "        max_sentences=None,\n",
        "        max_positions=utils.resolve_max_positions(\n",
        "            task.max_positions(),\n",
        "            max_tokens,\n",
        "        ),\n",
        "        ignore_invalid_inputs=True,\n",
        "        seed=seed,\n",
        "        num_workers=num_workers,\n",
        "        epoch=epoch,\n",
        "        disable_iterator_cache=not cached,\n",
        "        # Set this to False to speed up. However, if set to False, changing max_tokens beyond \n",
        "        # first call of this method has no effect. \n",
        "    )\n",
        "    return batch_iterator\n",
        "\n",
        "def try_load_checkpoint(model, optimizer=None, name=None):\n",
        "    name = name if name else \"checkpoint_last.pt\"\n",
        "    checkpath = Path(config.savedir)/name\n",
        "    if checkpath.exists():\n",
        "        check = torch.load(checkpath)# （取消comment時記得把左邊括號刪掉）, map_location=torch.device('cpu'))\n",
        "        model.load_state_dict(check[\"model\"])\n",
        "        stats = check[\"stats\"]\n",
        "        step = \"unknown\"\n",
        "        if optimizer != None:\n",
        "            optimizer._step = step = check[\"optim\"][\"step\"]\n",
        "        logger.info(f\"loaded checkpoint {checkpath}: step={step} loss={stats['loss']} bleu={stats['bleu']}\")\n",
        "    else:\n",
        "        logger.info(f\"no checkpoints found at {checkpath}!\")\n",
        "\n",
        "from NoamOpt import *\n",
        "\n",
        "## set args\n",
        "arch_args = Namespace(\n",
        "    encoder_embed_dim=256,\n",
        "    encoder_ffn_embed_dim=1024,\n",
        "    encoder_layers=4,\n",
        "    decoder_embed_dim=256,\n",
        "    decoder_ffn_embed_dim=1024,\n",
        "    decoder_layers=4,\n",
        "    share_decoder_input_output_embed=True,\n",
        "    dropout=0.3,\n",
        ")\n",
        "def add_transformer_args(args):\n",
        "    args.encoder_attention_heads=4\n",
        "    args.encoder_normalize_before=True\n",
        "    \n",
        "    args.decoder_attention_heads=4\n",
        "    args.decoder_normalize_before=True\n",
        "    \n",
        "    args.activation_fn=\"relu\"\n",
        "    args.max_source_positions=1024\n",
        "    args.max_target_positions=1024\n",
        "    \n",
        "    # 補上我們沒有設定的Transformer預設參數\n",
        "    from fairseq.models.transformer import base_architecture \n",
        "    base_architecture(arch_args)\n",
        "\n",
        "add_transformer_args(arch_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lP4-xxJ4QWyS"
      },
      "outputs": [],
      "source": [
        "from classSeq2Seq import *\n",
        "model = build_model(arch_args, task)\n",
        "logger.info(model)\n",
        "\n",
        "optimizer = NoamOpt(\n",
        "    model_size=arch_args.encoder_embed_dim, \n",
        "    factor=config.lr_factor, \n",
        "    warmup=config.lr_warmup, \n",
        "    optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))\n",
        "plt.plot(np.arange(1, 100000), [optimizer.rate(i) for i in range(1, 100000)])\n",
        "plt.legend([f\"{optimizer.model_size}:{optimizer.warmup}\"])\n",
        "\n",
        "try_load_checkpoint(model, optimizer, name=config.resume)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNSkhVmyRDYD"
      },
      "outputs": [],
      "source": [
        "torch.load('./checkpoints/transformer/checkpoint13.pt')#（刪掉左邊括號）, map_location=torch.device('cpu'))\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsiCWFH0BnCD"
      },
      "outputs": [],
      "source": [
        "## 刪掉上次predict可能殘存的test檔\n",
        "to_del_raw = ['test.en', 'test.raw.en', 'test.raw.clean.en']\n",
        "for f in to_del_raw:\n",
        "  if Path(f'./DATA/rawdata/ted2020/{f}').exists():\n",
        "    !rm ./DATA/rawdata/ted2020/{f}\n",
        "    print(f'{f} is deleted now.')\n",
        "  else:\n",
        "    print(f'{f} does not exists.')\n",
        "    \n",
        "## 這邊上傳要預測的文字檔（summarized）\n",
        "!cp './summary_out_put.txt' './DATA/rawdata/ted2020/test.raw.en' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* 把摘要模型的output，複製一份作為翻譯模型的input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtD20xJEkw78"
      },
      "outputs": [],
      "source": [
        "!cp './summary_out_put.txt' './DATA/rawdata/ted2020/test.raw.en' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EFuw-ajgzpD"
      },
      "outputs": [],
      "source": [
        "# 把test.zh改成跟test.en一樣多行\n",
        "with open('./DATA/rawdata/ted2020/test.raw.en', 'r') as en:\n",
        "  length = len(en.readlines())\n",
        "  tmp = open('./DATA/rawdata/ted2020/test.zh', 'r')\n",
        "  repeat = tmp.readline()\n",
        "  tmp.close()\n",
        "  with open('./DATA/rawdata/ted2020/test.zh', 'w') as zh:\n",
        "    for i in range(length):\n",
        "      zh.write(repeat)    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 欲翻譯文本的前處理\n",
        "* 清除多餘標點符號\n",
        "* 斷成subwords，以孑孓常遇到未登錄詞的問題\n",
        "* binarize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsjeynGa2Z3y"
      },
      "outputs": [],
      "source": [
        "from cleanse import *\n",
        "clean_corpus('./DATA/rawdata/ted2020/test.raw', 'en', 'zh', ratio=-1, min_len=-1, max_len=-1)\n",
        "spm_model = spm.SentencePieceProcessor(model_file=str('./DATA/rawdata/ted2020/spm8000.model'))\n",
        "\n",
        "# input的資料\n",
        "if Path('./DATA/rawdata/ted2020/test.en').exists():\n",
        "  print('資料已經轉成subwords，跳過此步驟')\n",
        "else:\n",
        "  with open('./DATA/rawdata/ted2020/test.en', 'w+') as out_f:\n",
        "    with open('./DATA/rawdata/ted2020/test.raw.clean.en' ,'r') as f:\n",
        "      for line in f:\n",
        "        line = line.strip()\n",
        "        tok = spm_model.encode(line, out_type=str)\n",
        "        print(' '.join(tok), file=out_f)\n",
        "# !head {'./DATA/rawdata/ted2020/test.en'} -n 10\n",
        "\n",
        "## 把上次binarize的bin清除\n",
        "p = './DATA/data-bin/ted2020/'\n",
        "ToDelEveryPred = ['test.en-zh.en.bin', 'test.en-zh.en.idx', 'test.en-zh.zh.bin', 'test.en-zh.zh.idx']\n",
        "for f in ToDelEveryPred:\n",
        "  if Path(p + f).exists():\n",
        "    !rm {p}{f}\n",
        "    print(f'{f} is deleted now')\n",
        "\n",
        "## binarize\n",
        "binpath = Path('./DATA/data-bin', 'ted2020')\n",
        "!python -m fairseq_cli.preprocess \\\n",
        "    --source-lang 'en'\\\n",
        "    --target-lang 'zh'\\\n",
        "    --testpref './DATA/rawdata/ted2020/test'\\\n",
        "    --destdir {binpath}\\\n",
        "    --srcdict './DATA/data-bin/ted2020/dict.en.txt' \\\n",
        "    --tgtdict './DATA/data-bin/ted2020/dict.zh.txt' \\\n",
        "    --workers 2\n",
        "\n",
        "#################################################################\n",
        "\n",
        "sequence_generator = task.build_generator([model], config)\n",
        "\n",
        "def decode(toks, dictionary):\n",
        "    # 從 Tensor 轉成人看得懂的句子\n",
        "    s = dictionary.string(\n",
        "        toks.int().cpu(),\n",
        "        config.post_process,\n",
        "    )\n",
        "    return s if s else \"<unk>\"\n",
        "\n",
        "def inference_step(sample, model):\n",
        "    gen_out = sequence_generator.generate([model], sample)\n",
        "    srcs = []\n",
        "    hyps = []\n",
        "    refs = []\n",
        "    for i in range(len(gen_out)):\n",
        "        # 對於每個 sample, 收集輸入，輸出和參考答案，稍後計算 BLEU\n",
        "        srcs.append(decode(\n",
        "            utils.strip_pad(sample[\"net_input\"][\"src_tokens\"][i], task.source_dictionary.pad()), \n",
        "            task.source_dictionary,\n",
        "        ))\n",
        "        hyps.append(decode(\n",
        "            gen_out[i][0][\"tokens\"], # 0 代表取出 beam 內分數第一的輸出結果\n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "        refs.append(decode(\n",
        "            utils.strip_pad(sample[\"target\"][i], task.target_dictionary.pad()), \n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "    return srcs, hyps, refs\n",
        "\n",
        "\n",
        "def generate_prediction(model, task, split=\"test\", outfile=\"./prediction.txt\"):    \n",
        "    task.load_dataset(split=split, epoch=1)\n",
        "    itr = load_data_iterator(task, split, 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
        "    idxs = []\n",
        "    hyps = []\n",
        "\n",
        "    model.eval()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"prediction\")\n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(progress):\n",
        "            # validation loss\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "\n",
        "            # 進行推論\n",
        "            s, h, r = inference_step(sample, model)\n",
        "            \n",
        "            hyps.extend(h)\n",
        "            idxs.extend(list(sample['id']))\n",
        "            \n",
        "    # 根據 preprocess 時的順序排列\n",
        "    hyps = [x for _,x in sorted(zip(idxs,hyps))]\n",
        "    print(hyps)\n",
        "    with open(outfile, \"w\") as f:\n",
        "            for i in hyps:\n",
        "              f.write(i+\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 進行預測"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1-HZs1bX4Ho"
      },
      "outputs": [],
      "source": [
        "generate_prediction(model, task)\n",
        "p = './DATA/data-bin/ted2020/'\n",
        "ToDelEveryPred = ['test.en-zh.en.bin', 'test.en-zh.en.idx', 'test.en-zh.zh.bin', 'test.en-zh.zh.idx']\n",
        "for f in ToDelEveryPred:\n",
        "  if Path(p + f).exists():\n",
        "    !rm {p}{f}\n",
        "    print(f'{f} is deleted now')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc4x5-cZEQ_w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ir7hEWSB5n2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "load & predict.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
